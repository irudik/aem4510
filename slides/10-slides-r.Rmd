---
title: "Lecture 10"
subtitle: "R and the tidyverse // regression"
author: Ivan Rudik
date: AEM 4510
output:
  xaringan::moon_reader:
    css: ['default', 'metropolis', 'metropolis-fonts', 'my-css.css']
    # self_contained: true
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'

---
exclude: true
```{r setup}
if (!require("pacman")) install.packages("pacman")
pacman::p_load(
  tidyverse, xaringanExtra, rlang, patchwork, nycflights13, broom, viridis
)
options(htmltools.dir.version = FALSE)
knitr::opts_hooks$set(fig.callout = function(options) {
  if (options$fig.callout) {
    options$echo <- FALSE
  }
knitr::opts_chunk$set(echo = TRUE, fig.align="center")
  options
})
red_pink <- "#e64173"
# A blank theme for ggplot
theme_empty <- theme_minimal() +
  theme(
    legend.position = "none",
    title = element_text(size = 24),
    axis.text.x = element_text(size = 24), axis.text.y = element_text(size = 24, color = "#eeeeee"),
    axis.title.x = element_text(size = 24), axis.title.y = element_text(size = 24),
    panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(),
    panel.background = element_rect(fill = "#eeeeee", colour = NA),
    plot.background = element_rect(fill = "#eeeeee", colour = NA),
    axis.line = element_line(colour = "black"), axis.ticks = element_line(),
  )
theme_blank <- theme_minimal() +
  theme(
    legend.position = "none",
    title = element_text(size = 24),
    axis.text.x = element_blank(), axis.text.y = element_blank(),
    axis.title.x = element_blank(), axis.title.y = element_blank(),
    panel.grid.minor.x = element_blank(), panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(), panel.grid.major.x = element_blank(),
    panel.background = element_rect(fill = "#eeeeee", colour = NA),
    plot.background = element_rect(fill = "#eeeeee", colour = NA),
    axis.line = element_blank(), axis.ticks = element_blank(),
  ) 
```
```{r xaringanExtra, echo = FALSE}
xaringanExtra::use_xaringan_extra(c("tile_view", "panelset", "webcam"))

```
```{r echo=FALSE}
xaringanExtra::style_panelset(panel_tab_color_active = "red")
```

---

# Roadmap

- What is R?
- What is the tidyverse?
- How do we import and manipulate data?

Our goal is to take a hands on approach to learning how we actually .hi-blue[do] environmental economics

A good chunk of this lecture comes from Grant Mcdermott's [data science for economists](https://github.com/uo-ec607/lectures) notes, Ed Rubin's [intro to econometrics](https://github.com/edrubin/EC421S20), and [RStudio education](https://education.rstudio.com/)

---

class: inverse, center, middle
name: r

# RStudio Cloud

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Getting started

We will be using [rstudio.cloud](https://rstudio.cloud) for our coding

--

Why?

--

You don't need to download/install anything

--

I can prepare the packages and code and make it easy to download

--

Let's get everything going...

---

# Getting started: login 

![](files/login.png)

---

# Getting started: new project

![](files/10-new-project.png)


---

# Getting started: wait for deployment

![](files/deploying.png)

---

# Click on base-code in bottom-right

![](files/10-open-base-code.png)


---

# Code script open!

![](files/10-coding-window.png)

---

# Now we're set

Now we're all set with our coding environment

--

You can write code in the top window and save it as a file

--


Or you can just enter it in the console in the bottom if you don't want to save it

--

Highlight code in the top window and press cmd+enter to run those highlighted lines

---

class: inverse, center, middle
name: r

# Quick intro to R

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# What is R?

What is R?

> R is a language and environment for statistical computing and graphics.

What is RStudio?

> RStudio is an integrated development environment (IDE) for R, a programming language for statistical computing and graphics.

--

Basically: R is the coding language, RStudio is the graphical interface


---

# Why are we using R?

1. R is free

--

2. R is relatively easy to learn (especially with `tidyverse` which we will be using)

--

3. R is widely used for statistical analysis and data science in business, economics, natural sciences

--

4. R has a large online community for help (e.g. StackOverflow), and lots and lots of packages that help you do your analysis smoothly

---

# Let's see what we can do in R

Next let's see how to use R

--

First we will start with the basics

--

Then we will start doing more complicated exercises that are important for doing data analysis and economics work

--

Please follow along in RStudio/RStudio Cloud

---

# Arithmetic operations

R can do all the standard arithmetic operations

```{r}
1+2 ## add
6-7 ## subtract
5/2 ## divide
```

---

# Logical operations

You also have logical operations

```{r}
1 > 2
(1 > 2) | (1 > 0.5) # | is the or operator
(1 > 2) & (1 > 0.5) # & is the and operator
```

---

# Logical operations

We can negate expressions with: `!`

This is helpful for filtering data

```{r}
is.na(1:10)
!is.na(1:10)
```

`NA` means .hi[not available] (i.e. missing)
---

# Logical operators

For value matching we use: `%in%`

To see whether an object is contained within (i.e. matches one of) a list of items, use `%in%`.
```{r}
4 %in% 1:10
4 %in% 5:10
```

This is kind of like an `any` command in other languages

---

# Logical operators 

To evaluate whether two expressions are equal, we need to use .hi-red[two] equal signs

```{r, error=T}
1 = 1 ## This doesn't work
1 == 1 ## This does.
1 != 2 ## Note the single equal sign when combined with a negation.
```


---

# Assignment

In R, we can use either `=` or `<-` to handle assignment<sup>1</sup>

.footnote[
<sup>1</sup> The `<-` is really a `<` followed by a `-`. It just looks like an arrow because of the font on the slides
]

--

`<-` is normally read aloud as "gets"

You can think of it as a (left-facing) arrow saying .hi[assign in this direction]

---

# Assignment

```{r}
a <- 10 + 5
a
```

---

# Assignment

You can also use `=` for assignment

```{r}
b = 10 + 10 
b
```

---

# Which assignment operator to use?

Most R folks prefer `<-` for assignment

--

It doesn't really matter though, other languages strictly use `=`

--

.hi[Use whatever you prefer, just be consistent]

---

# Help

If you are struggling with a (named) function or object in R, simply type `?commandhere`
  
```R
?rowSums 
```

---

# Help 

Also try `vignette()` for a more detailed introduction to many packages

```R
# Try this:
vignette("dplyr")
```

--

Vignettes are a very easy way to learn how and when to use a package

---

# Object-oriented programming

In R:
> "Everything is an object and everything has a name."

---

# What are objects? 

We won't go into details but here are some objects that we'll be working with regularly:
- vectors
- matrices
- data frames
- lists
- etc.

--

A lot of these are probably familiar if you have coding experience

---

# Data frames

The most important object we will be working with is the .hi[data frame]

--

You can think of it basically as an Excel spreadsheet

--

```{r df}
## Create a small data frame called "df".
df <- data.frame(x = 1:2, y = 3:4) 
df
```

--

This is essentially just a table with columns named `x` and `y`

--

Each row is an observation telling us the values of both `x` and `y`

---

# Working with multiple objects

In R we can have multiple data frames in memory at once

```{r df2}
df2 <- data.frame(x = rnorm(10), y = rnorm(10)) # x and y are each 10 normal random numbers
df # df still exists even though we made df2!
df2 # here's df2
```


---

# Built in dataframes

R (and its packages) also has a bunch of built in dataframes with special names that you can call upon any time, we will be using these for examples

```{r}
mtcars
```

---

# Built in dataframes

R (and its packages)  also has a bunch of built in dataframes with special names that you can call upon any time, we will be using these for examples

This one is in the `dplyr` package that we will load later

```{r}
starwars
```

---

# Reserved words

R has a bunch of key/reserved words that serve specific functions

See [here](http://stat.ethz.ch/R-manual/R-devel/library/base/html/Reserved.html) for a full list, including (but not limited to):

```R
if
else 
while # looping
function 
for # looping
TRUE 
FALSE 
NULL # null/undefined
Inf # infinity
NaN # Not a number
NA # Not available / missing
```

---

# Semi-reserved words

There are other words that are sort of reserved, in that they have a particular meaning

The most important one is `c` which binds and concatenates objects together

```{r}
my_vector <- c(1, 2, 5)
my_vector
```

This created a vector/row consisting of 1, 2, 5

---

# Namespace conflicts

Try loading up `dplyr` in RStudio
```{r, warning = T, tidy = T}
library(dplyr)
```

What warning gets reported?

--

The warning *masked from 'package:X'* is about a .hi-red[namespace conflict]

--

`dplyr` and the `stats` package (which gets loaded automatically when you start R) have functions named `filter` and `lag`

---

# Namespace conflicts 

Whenever a namespace conflict arises, the most recently loaded package will gain preference

--

The `filter()` function now refers specifically to the `dplyr` variant

--

But what if we want the `stats` variant?

--

1. Temporarily use `stats::filter()`
2. Permanently assign `filter <- stats::filter`

---

# Solving namespace conflicts

Use `package::function()`

--

Explicitly call a conflicted function from a package using the `package::function()` syntax:
```{r}
stats::filter(1:10, rep(1, 2))
```

---

# Solving namespace conflicts

We can also use `::` for more than just conflicted cases.

--

It can clarify where a function or dataset comes from and make the code clearer:
```r
dplyr::starwars ## Print the starwars data frame from the dplyr package
scales::comma(c(1000, 1000000)) ## Use the comma function, which comes from the scales package
```

--

The `::` syntax also means that we can call functions without loading package first. E.g. As long as `dplyr` is installed on our system, then `dplyr::filter(iris, Species=="virginica")` will work.
  
---

# Solving namespace conflicts 

Assign `function <- package::function`
  
--

You can permanently assign a conflicted function name to a particular package

--

This will hold for the remainder of your current R session, or until you change it back:
```{r, eval = F}
filter <- stats::filter ## Note the lack of parentheses.
filter <- dplyr::filter ## Change it back again.
```

---

# Indexing

How do we index in R?

--

We've already seen an example of indexing in the form of R console output:
```{r}
1+2
```

The `[1]` above denotes the first (and, in this case, only) element of our output

--

In this case, a vector of length one equal to the value "3"

---

# Indexing

Try the following in your console to see a more explicit example of indexed output:
```{r}
rnorm(n = 100, mean = 0, sd = 1) # 100 random variables with mean 0 standard deviation 1
```

---

# Indexing: []


We can also use `[]` to index objects that we create in R.
```{r}
a <- 11:20
a
a[4] ## Get the 4th element of object "a"
a[c(4, 6)] ## Get the 4th and 6th elements
```


---

# Indexing: []

It also works on larger arrays (vectors, matrices, data frames, and lists). For example:
```{r}
starwars[1, 1] ## Show the cell corresponding to the 1st row & 1st column of the data frame.
```

--

What does `starwars[1:3, 1]` give you?

---

# Indexing: []

We haven't covered them properly yet (patience), but .hi-blue[lists] are a more complex type of array object in R

--

They can contain a random assortment of objects that don't share the same characteristics

--

- e.g. a list can contain a scalar, a string, and a data frame, or even another list

---

# Indexing: []

The relevance to indexing is that lists require two square brackets `[[]]` to index the parent list item and then the standard `[]` within that parent item:
```{r my_list, cache=T}
my_list <- list(
  a = "hello", 
  b = c(1,2,3), 
  c = data.frame(x = 1:5, y = 6:10)
  )
my_list[[1]] ## Return the 1st list object
my_list[[2]][3] ## Return the 3rd element of the 2nd list object
```

---

# Indexing: $


Lists provide a nice segue to our other indexing operator: `$`
- Let's continue with the `my_list` example from the previous slide.

```{r}
my_list
```

---
count: false

# Indexing: $


Lists provide a nice segue to our other indexing operator: `$`.
- Let's continue with the `my_list` example

```{r, eval=F}
my_list
```

```
*## $a
## [1] "hello"
## 
*## $b
## [1] 1 2 3
## 
*## $c
##   x  y
## 1 1  6
## 2 2  7
## 3 3  8
## 4 4  9
## 5 5 10
```

Notice how our (named) parent list objects are demarcated: "$a", "$b" and "$c".

---

# Indexing: $

We can call these objects directly by name using the dollar sign, e.g.
```{r}
my_list$a ## Return list object "a"
my_list$b[3] ## Return the 3rd element of list object "b" 
my_list$c$x ## Return column "x" of list object "c"
```

---

# Indexing: $

The `$` form of indexing also works for other object types

In some cases, you can also combine the two index options:
```{r}
starwars$name[1] # first element of the name column of the starwars data frame
```

---

# Indexing: $

However, note some key differences between the output from this example and that of our previous `starwars[1, 1]` example:
```{r}
starwars$name[1]
starwars[1, 1]
```

---

# Removing objects 

Use `rm()` to remove an object or objects from your working environment.
```{r}
a <- "hello"
b <- "world"
rm(a, b)
```

You can also use `rm(list = ls())` to remove all objects in your working environment (except packages), or just start a new R session instead

---

class: inverse, center, middle
name: tidyverse

# The tidyverse

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# What is "tidy" data?

What we are going to learn is how to use a set of packages called the .hi[tidyverse]

--

These sets of packages make working with data .hi-blue[extremely easy] and .hi-blue[intuitive]

---

# What is "tidy" data?

Resources:
- [Vignette](https://cran.r-project.org/web/packages/tidyr/vignettes/tidy-data.html) (from the **tidyr** package)
- [Original paper](https://vita.had.co.nz/papers/tidy-data.pdf) (Hadley Wickham, 2014 JSS)

--

Key points:
1. Each variable forms a column.
2. Each observation forms a row.
3. Each type of observational unit forms a table.

--

Basically, tidy data is more likely to be [long (i.e. narrow)](https://en.wikipedia.org/wiki/Wide_and_narrow_data) than wide

---

# Checklist

Install tidyverse: `install.packages('tidyverse')` (already done on cloud)

Install nycflights13: `install.packages('nycflights13', repos = 'https://cran.rstudio.com')`

---

# Tidyverse vs. base R

Lots of debate over tidyverse vs base R

--

The answer is [obvious](http://varianceexplained.org/r/teach-tidyverse/): We should teach the tidyverse first
- Good documentation and support
- Consistent philosophy and syntax
- Nice front-end for big data tools
- For data cleaning, plotting, the tidyverse is elite

---

# Tidyverse vs. base R

Base R is still great
- Base R is extremely flexible and powerful
- The tidyverse can't do everything
- Using base R and the tidyverse together is often a good idea

---

# Tidyverse vs. base R

One point of convenience is that there is often a direct correspondence between a tidyverse command and its base R equivalent:

| tidyverse  |  base |
|---|---|
| `?readr::read_csv`  | `?utils::read.csv` |
|  `?dplyr::if_else` |  `?base::ifelse` |
|  `?tibble::tibble` |  `?base::data.frame` |
  
Tidyverse functions typically have extra features on top of base R

--

There are always many ways to achieve a single goal in R

---

# Tidyverse packages

Let's load the tidyverse meta-package and check the output.
```{r tverse, cache = FALSE}
library(tidyverse)
```

--

We have actually loaded a number of packages: **ggplot2**, **tibble**, **dplyr**, etc

--

We can also see information about the package versions and some [namespace conflicts](https://raw.githack.com/uo-ec607/lectures/master/04-rlang/04-rlang.html#59)

---

# Tidyverse packages

The tidyverse actually comes with a lot more packages than those that are just loaded automatically
```{r tverse_pkgs}
tidyverse_packages()
```

e.g. the **lubridate** package is for working with dates and the **rvest** package is for webscraping

--

These packages have to be loaded separately

---

# Tidyverse packages

We're going to focus on two workhorse packages:
1. [**dplyr**](https://dplyr.tidyverse.org/)
2. [**tidyr**](https://tidyr.tidyverse.org/)

These are the packages for cleaning and wrangling data

--

They are thus the ones that you will likely make the most use of

--

Data cleaning and wrangling is important and knowing how to do it well is a good skill for any data-oriented job

---

# Pipes: %>%

The pipe operator `%>%` lets us perform a sequence of operations in a very nice and tidy way

--

Let's consider a fake example to get the idea for why its really beneficial

---

# Pipes: %>%

Let's say we wanted to apply a sequence of operations that tells the computer what you did throughout your day:

1. Wake up
2. Get out of bed
3. Comb hair
4. Go downstairs
5. Drink a cup
6. Grab hat
7. Catch bus

---

# Pipes: %>%

If you were to code this up in a traditional way it might look one of two ways:

A bunch of lines doing all the different steps
```{r, eval = F}
me <- wake_up(me)
me <- get_out_of_bed(me)
me <- comb_hair(me)
me <- go(me, "downstairs")
me <- drink(me, "cup")
me <- grab(me, "hat")
me <- catch(me, "bus")
```

---

# Pipes: %>%

If you were to code this up in a traditional way it might look one of two ways:

Or if you're a little crazy then do it all in one line
```{r, eval = F}
me <- catch(grab(drink(go(comb_hair(get_out_of_bed(wake_up(me))), where = "downstairs"), what = "cup"), what = "hat"), what = "bus")
```

These are kind of tedious or messy and out of the order you'd think

---

# Pipes: %>%

With pipes we can do everything at once, but have it be .hi[in order]:

```{r, eval = F}
me <- me %>%
  wake_up() %>%
  get_out_of_bed() %>% 
  comb_hair() %>% 
  go("downstairs") %>%
  drink("cup") %>% 
  grab("hat") %>% 
  catch("bus")
```

This makes everything .hi[very intuitive] to read and code!


---

# Pipes: %>%

Here's a real example: suppose we wanted to figure out the average highway miles per gallon of Audi's in the `mpg` dataset:
```{r}
mpg
```

---

# Pipes: %>%

There's two ways you might do this without taking advantage of pipes:

--

The first is to do it step-by-step, line-by-line which requires a lot of variable assignment

```{r}
audis_mpg <- filter(mpg, manufacturer=="audi")
audis_mpg_grouped <- group_by(filter(mpg, manufacturer=="audi"), model)
summarise(audis_mpg_grouped, hwy_mean = mean(hwy))
```

---

# Pipes: %>%

Next you could do it all in one line which is hard to read

```{r}
summarise(group_by(filter(mpg, manufacturer=="audi"), model), hwy_mean = mean(hwy))
```

---

# Pipes: %>%

Or, you could use .hi-blue[pipes] `%>%`:

```{r}
mpg %>% filter(manufacturer=="audi") %>% group_by(model) %>% summarise(hwy_mean = mean(hwy))
```

--

It performs the operations from left to right, exactly like you'd think of them: take this object (mpg), do this (filter), then do this (group by car model), then do this (take the mean of highway miles)


---

# Use vertical space

Pipes are even more readable if we write it over several lines:
```{r pipe}
mpg %>% 
  filter(manufacturer=="audi") %>% 
  group_by(model) %>% 
  summarise(hwy_mean = mean(hwy))
```

Using vertical space costs nothing and makes for much more readable code 

---
class: inverse, center, middle
name: dplyr

# dplyr

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Aside: dplyr 1.0.0 release

Please make sure that you are running at least **dplyr** 1.0.0 before continuing.

```{r dplyr_vers, cache=FALSE}
packageVersion("dplyr")
# install.packages('dplyr') ## install updated version if < 1.0.0
```


---

# The five key dplyr verbs

1. `filter`: Subset/filter rows based on their values

2. `arrange`: Reorder/arrange rows based on their values

3. `select`: Select columns/variables

4. `mutate`: Create new columns/variables

5. `summarise`: Collapse multiple rows into a single summary value, potentially by a grouping variable

--

Let's practice these commands together using the `starwars` data frame that comes pre-packaged with dplyr

---

# Starwars

Here's the `starwars` dataset, it has 87 observations of 14 variables
```{r starwars}
starwars 
```


---

# 1) dplyr::filter

Why filter?

--

Filtering subsets your data

--

This means that you can take out data that meet certain characteristics

--

e.g. if you want to run your analysis only on low income areas, or if you want to focus on years after 2005

---

# 1) dplyr::filter

Here we are subsetting the observations of humans that are at least 190cm
```{r filter1}
starwars %>% 
  filter( 
    species == "Human", 
    height >= 190
    ) 
```

---

# 1) dplyr::filter

You can filter using regular expressions with grep-type commands or the `stringr` package
```{r filter2}
starwars %>% 
  filter(stringr::str_detect(name, "Skywalker"))
```

This subsets the observations for individuals whose names contain "Skywalker"

---

# 1) dplyr::filter

A very common `filter` use case is identifying/removing missing data cases:
```{r filter3}
starwars %>% 
  filter(is.na(height))
```

---

# 1) dplyr::filter

To remove missing observations, use negation:
```{r filter4}
starwars %>% 
  filter(!is.na(height))
```

---

# 2) dplyr::arrange

`arrange` sorts the data frame based on the variables you supply:
```{r arrange1}
starwars %>% 
  arrange(birth_year)
```

---

# 1) dplyr::arrange

Why arrange?

--

Arrange sorts your data

--

This makes it easy to check and see patterns in the data

---

# 2) dplyr::arrange

We can also arrange items in descending order using `arrange(desc())`
```{r arrange2}
starwars %>% 
  arrange(desc(birth_year))
```

---

# 3) dplyr::select

Why select?

--

Select lets you choose columns to keep or drop

--

This means you are essentially getting rid of variables, likely ones you do not need

--

e.g. if you used an emissions rate and marginal damage per unit of pollution variable to construct the marginal damage of output, you may not need the first two variables any more

---

# 3) dplyr::select

Use commas to select multiple columns out of a data frame, deselect a column with "-", select across multiple columns with "first:last":
```{r select1}
starwars %>% 
  select(name:skin_color, species, -height)
```

---

# 3) dplyr::select 

You can also rename your selected variables in place
```{r select2}
starwars %>%
  select(alias = name, crib = homeworld) 
```

---

# 3) dplyr::select 

If you just want to rename columns without subsetting them, you can use `rename`:
```{r}
starwars %>%
  rename(alias = name, crib = homeworld) 
```

---

# 3) dplyr::select *cont.*

The `select(contains(PATTERN))` option provides a nice shortcut in relevant cases.
```{r select3}
starwars %>% 
  select(name, contains("color"))
```

---

# 3) dplyr::select

The `select(..., everything())` option is another useful shortcut if you only want to bring some variable(s) to the "front" of a data frame

```{r select4}
starwars %>% 
  select(species, homeworld, everything()) %>%
  head(5)
```

---

# 3) dplyr::select

You can also use `relocate` to do the same thing

```{r}
starwars %>% 
  relocate(species, homeworld) %>%
  head(5)
```

---

# 4) dplyr::mutate

Why mutate?

--

You may need to create new variables

--

e.g. if I give you nominal house prices and the rate of house price inflation, you need to combine these two things to make a new .hi[real house price] variable

---

# 4) dplyr::mutate

You can create new columns from scratch as transformations of existing columns:
```{r mutate1}
starwars %>% 
  select(name, birth_year) %>%
  mutate(dog_years = birth_year * 7) %>%
  mutate(comment = paste0(name, " is ", dog_years, " in dog years."))
```

---


# 4) dplyr::mutate 

*Note:* `mutate` creates variables in order, so you can chain multiple mutates in a single call
```{r mutate2}
starwars %>% 
  select(name, birth_year) %>%
  mutate(
    dog_years = birth_year * 7, ## Separate with a comma
    comment = paste0(name, " is ", dog_years, " in dog years.")
    )
```

---

# 4) dplyr::mutate 

Boolean, logical and conditional operators all work well with `mutate` too:
```{r mutate3}
starwars %>% 
  select(name, height) %>%
  filter(name %in% c("Luke Skywalker", "Anakin Skywalker")) %>% 
  mutate(tall1 = height > 180) %>% # TRUE or FALSE
  mutate(tall2 = ifelse(height > 180, "Tall", "Short")) ## Same effect, but can choose labels
```

---

# 4) dplyr::mutate 

Lastly, combining `mutate` with `across` allows you to easily work on a subset of variables:
```{r, mutate4}
starwars %>% 
  select(name:eye_color) %>% 
  mutate(across(where(is.character), toupper)) %>% # Take all character variables, uppercase them
  head(5)
```

---

# 5) dplyr::summarise

Why summarise?

--

Often we want to get summary statistics or *collapse* our data

--

e.g. if I gave you a data set of each individual's marginal damage in the US, we may want to aggregate up to county-level marginal damages

---

# 5) dplyr::summarise

Summarising useful in combination with the `group_by` command
```{r summ1}
starwars %>% 
  group_by(species, gender) %>% # for each species-gender combo
  summarise(mean_height = mean(height, na.rm = TRUE)) # calculate the mean height
```

---

# 5) dplyr::summarise 

Note that including "na.rm = TRUE" is usually a good idea with summarise functions, it keeps NAs from propagating to the end result
```{r summ2}
## Probably not what we want
starwars %>% 
  summarise(mean_height = mean(height))
```

---

# 5) dplyr::summarise

We can also use `across` within summarise:
```{r, summ4}
starwars %>% 
  group_by(species) %>% # for each species
  summarise(across(where(is.numeric), mean, na.rm = T)) %>% # take the mean of all numeric variables
  head(5)
```

---

# Other dplyr goodies

`group_by` and `ungroup`: For (un)grouping
- Particularly useful with the `summarise` and `mutate` commands

--

`slice`: Subset rows by position rather than filtering by values
- E.g. `starwars %>% slice(c(1, 5))`

---

# Other dplyr goodies

`pull`: Extract a column from as a data frame as a vector or scalar
- E.g. `starwars %>% filter(gender=="female") %>% pull(height)`

--

`count` and `distinct`: Number and isolate unique observations
- E.g. `starwars %>% count(species)`, or `starwars %>% distinct(species)`


---

# Other dplyr goodies

There are also a whole class of [window functions](https://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html) for getting leads and lags, percentiles, cumulative sums, etc.
- See `vignette("window-functions")`.

---

# dplyr::xxxx_join

The last set of commands we need are the `join` commands

--

These are the same as `merge` in stata but with a bit more functionality

--

Why join?

--

Suppose we want to understand how pollution affects housing prices

--

You may need to combine data sets: one data sets on house prices in all counties, and another data set on pollution levels in all counties

---

# dplyr::xxxx_join

How does joining work?

--

We need two datasets (e.g. housing prices and pollution)

--

Each dataset has the same set of .hi[key] variables (e.g. county)

--

When we join, we match up the two datasets on the key, and combine them into one

--

In our housing example, each row of the joined dataset would now tell us the house price, the county its in, and the county's pollution


---

# dplyr::xxxx_join

We merge data with [join operations](https://cran.r-project.org/web/packages/dplyr/vignettes/two-table.html):
- `inner_join(df1, df2)`
- `left_join(df1, df2)`
- `right_join(df1, df2)`
- `full_join(df1, df2)`

(You can visualize the operations [here](https://r4ds.had.co.nz/relational-data.html))

---

# dplyr::xxxx_join

Lets use the data that comes with the the [nycflights13](http://github.com/hadley/nycflights13) package. 

```{r}
library(nycflights13)
flights 
```

---

# dplyr::xxxx_join

```{r}
planes
```

---

# Joining operations

Let's perform a left join on the flights and planes datasets
- *Note*: I'm going subset columns after the join, but only to keep text on the slide

--

```{r join1}
left_join(flights, planes) %>%
  select(year, month, day, dep_time, arr_time, carrier, flight, tailnum, type, model)
```

---

# Joining operations

Note that dplyr made a reasonable guess about which columns to join on (i.e. columns that share the same name), and told us what it chose
```
*## Joining, by = c("year", "tailnum")
```

There's an obvious problem here: the variable `year` does not have a consistent meaning across our joining datasets

--

In one it refers to the *year of flight*, in the other it refers to *year of construction*

Luckily, there's an easy way to avoid this problem: try `?dplyr::join`

---

# Joining operations 

You just need to be more explicit in your join call by using the `by = ` argument 
```{r join2}
left_join(
  flights,
  planes %>% rename(year_built = year), ## Not necessary w/ below line, but helpful
  by = "tailnum" ## Be specific about the joining column
  ) %>%
  select(year, month, day, dep_time, arr_time, carrier, flight, tailnum, year_built, type, model) %>%
  head(3) ## Just to save vertical space on the slide
```

---

# Joining operations

Note what happens if we again specify the join column but don't rename the ambiguous `year`:
```{r join3}
left_join(flights, 
          planes, ## Not renaming "year" to "year_built" this time
          by = "tailnum") %>%
  select(contains("year"), month, day, dep_time, arr_time, carrier, flight, tailnum, type, model) %>%
  head(3)
```

--

Make sure you know what "year.x" and "year.y" are

---
class: inverse, center, middle
name: tidyr

# tidyr

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# Key tidyr verbs

1. `pivot_longer`: Pivot wide data into long format (i.e. "melt", "reshape long")

2. `pivot_wider`: Pivot long data into wide format (i.e. "cast", "reshape wide") 

3. `separate`: Split one column into multiple columns

4. `unite`: Combine multiple columns into one

--

Let's practice these verbs together in class
  
---

# 1) tidyr::pivot_longer

```{r pivot_longer1a}
stocks <- data.frame(
  time = as.Date('2009-01-01') + 0:1,
  stock_X = rnorm(2, 0, 1),
  stock_Y = rnorm(2, 0, 2),
  stock_Z = rnorm(2, 0, 4)
  )
stocks
```

We have 4 variables, the date and the stocks

How do we get this in tidy form?

---

# 1) tidyr::pivot_longer

```{r pivot_longer1b, eval = F}
stocks %>% pivot_longer(-time, names_to = "stock", values_to = "price")
```

We need to pivot the stock name variables `X, Y, Z` longer

1. Choose non-time variables: `-time`
2. Decide what variable holds the names: `names_to = "stock"`
3. Decide what variable holds the values: `values_to = "price"`

---

# 1) tidyr::pivot_longer

```{r pivot_longer1c}
stocks %>% pivot_longer(-time, names_to = "stock", values_to = "price")
```

---

# 1) tidyr::pivot_longer

Let's quickly save the "tidy" (i.e. long) stocks data frame for use on the next slide

```{r pivot_longer2}
tidy_stocks <- stocks %>% 
  pivot_longer(-time, names_to = "stock", values_to = "price")
```

---

# 2) tidyr::pivot_wider

```{r pivot_wider1, dependson=tidy_stocks}
tidy_stocks %>% pivot_wider(names_from = stock, values_from = price)
tidy_stocks %>% pivot_wider(names_from = time, values_from = price)
```

--

Note that the second example has effectively transposed the data
---

# 3) tidyr::separate

```{r sep1}
economists <- data.frame(name = c("Adam.Smith", "Paul.Samuelson", "Milton.Friedman"))
economists
economists %>% separate(name, c("first_name", "last_name")) 
```

--

This command is pretty smart. But to avoid ambiguity, you can also specify the separation character with `separate(..., sep=".")`

---

# 4) tidyr::unite

```{r unite1a}
gdp <- data.frame(
  yr = rep(2016, times = 4),
  mnth = rep(1, times = 4),
  dy = 1:4,
  gdp = rnorm(4, mean = 100, sd = 2)
  )
gdp 
```

---

# 4) tidyr::unite

```{r unite1b}
## Combine "yr", "mnth", and "dy" into one "date" column
gdp %>% unite(date, c("yr", "mnth", "dy"), sep = "-")
```

---

# 4) tidyr::unite 

Note that `unite` will automatically create a character variable:
```{r unite2}
gdp_u <- gdp %>% unite(date, c("yr", "mnth", "dy"), sep = "-") %>% as_tibble()
gdp_u
```

--

If you want to convert it to something else (e.g. date or numeric) then you will need to modify it using `mutate`

---

# 4) tidyr::unite 

```{r unite3, message=F}
library(lubridate)
gdp_u %>% mutate(date = ymd(date))
```


---

class: inverse, center, middle
name: tidyverse

# Regression and ordinary least squares

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>


---

# Why?

## Motivation

Let's start with a few .hi[basic, general questions]

--

1. What is the goal of econometrics?

2. Why do economists (or other people) study or use econometrics?

--

.hi[One simple answer:] Learn about the world using data


---

# Why?

## Example

GPA is an output from endowments (ability), and hours studied (inputs), and pollution exposure (externality)

--

One might hypothesize a model:
$\text{GPA}=f(I, P, \text{SAT}, H)$

where $H$ is hours studied, $P$ is pollution exposure, $\text{SAT}$ is SAT score and $\text{I}$ is family income

--

We expect that GPA will rise with some variables, and decrease with others

--

But who needs to _expect_?

--

We can test these hypotheses .hi[using a regression model]

---

# How?

We can write down a linear regression model of the relationship between GPA and (H, P, SAT, PCT):
$$ \text{GPA}_i = \beta_0 + \beta_1 I_i + \beta_2 P_i + \beta_3 \text{SAT}_i + \beta_4 H_i + \varepsilon_i $$

The left hand side of the equals sign is our .hi-blue[dependent variable] GPA

The right hand side of the equals sign contains all of our .hi-red[independent variables] (I, P, SAT, H), and an error term $\varepsilon_i$ (described later)

The subscript $i$ means that the variable contains the value for some person $i$ in our dataset where $i = 1,\dots,N$

---

# How?

$$ \text{GPA}_i = \beta_0 + \beta_1 I_i + \beta_2 P_i + \beta_3 \text{SAT}_i + \beta_4 H_i + \varepsilon_i $$

We are interested in how pollution P affects GPA

--

This is given by $\beta_2$

--

Notice that $\beta_2 = \frac{\partial\text{GPA}_i}{\partial\text{P}_i}$

--

$\beta_2$ tells us how GPA changes, given a 1 unit increase in pollution!

--

Our goal will be to estimate $\beta_2$, we denote estimates with hats: $\hat{\beta}_2$


---

# How?

How do we estimate $\beta_2$?

--

First, suppose we have a set of estimates for all of our $\beta$s, then we can *estimate* the GPA $(\widehat{GPA}_i)$ for any given person based on just (I, P, SAT, H):
$$\widehat{GPA}_i = \hat{\beta}_0 + \hat{\beta}_1 I_i + \hat{\beta}_2 P_i + \hat{\beta}_3 \text{SAT}_i + \hat{\beta}_4 H_i$$

---

# How?

We estimate the $\beta$s with .hi[linear regression], specifically ordinary least squares

.hi[Ordinary least squares:] choose all the $\beta$s so that the sum of squared errors between the *real* GPAs and model-estimated GPAs are minimized:
$$SSE = \sum_{i=1}^N (GPA_i - \widehat{GPA}_i)^2$$

--

Choosing the $\beta$s in this fashion gives us the best-fit line through the data

---

# How?


```{R, gen dataset, include = F, cache = T}
# Set population and sample sizes
n_p <- 100
n_s <- 10
# Set the seed
set.seed(12468)
# Generate data
pop_df <- tibble(
  i = 3,
  x = rnorm(n_p, mean = 5, sd = 1.5),
  e = rnorm(n_p, mean = 0, sd = 1),
  y = i + 0.5 * x + e,
  row = rep(1:sqrt(n_p), times = sqrt(n_p)),
  col = rep(1:sqrt(n_p), each = sqrt(n_p)),
  s1 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s2 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s))),
  s3 = sample(x = c(rep(T, n_s), rep(F, n_p - n_s)))
)
# Regressions
lm0 <- lm(y ~ x, data = pop_df)
lm1 <- lm(y ~ x, data = filter(pop_df, s1 == T))
lm2 <- lm(y ~ x, data = filter(pop_df, s2 == T))
lm3 <- lm(y ~ x, data = filter(pop_df, s3 == T))
# Simulation
set.seed(12468)
sim_df <- lapply(X = 1:1e3, FUN = function(x, size = n_s) {
  lm(y ~ x, data = pop_df %>% sample_n(size = size)) %>% tidy()
}) %>% do.call(rbind, .) %>% as_tibble()
```

---

# Simple example

Suppose we were only looking at GPA and family income I

```{R, ols vs lines 1, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


For any line $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$

```{R, vs lines 2, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
# geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#3C93DC", size = 2, alpha = 0.9) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


For any line $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, we can calculate errors: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 3, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 6
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#3C93DC", size = 2, alpha = 0.9) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


For any line $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, we can calculate errors: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 4, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 3
b1 <- 0.2
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#3C93DC", size = 2, alpha = 0.9) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


For any line $\left(\hat{y} = \hat{\beta}_0 + \hat{\beta}_1 x\right)$, we can calculate errors: $e_i = y_i - \hat{y}_i$

```{R, ols vs lines 5, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1)), size = 0.5, alpha = 0.2) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#3C93DC", size = 2, alpha = 0.9) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


SSE squares the errors $\left(\sum e_i^2\right)$: bigger errors get bigger penalties

```{R, ols vs lines 6, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- 10
b1 <- -0.8
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#3C93DC", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---
count: false
# Simple example


The OLS estimate is the combination of $\hat{\beta}_0$ and $\hat{\beta}_1$ that minimize SSE

```{R, ols vs lines 7, echo = F, dev = "svg", fig.height = 4.5, fig.width = 10}
# Define a function
y_hat <- function(x, b0, b1) {b0 + b1 * x}
# Define line's parameters
b0 <- lm0$coefficients[1]
b1 <- lm0$coefficients[2]
# The plot
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_segment(aes(x = x, xend = x, y = y, yend = y_hat(x, b0, b1), color = (y - y_hat(x, b0, b1))^2), size = 0.5, alpha = 0.8) +
geom_point(size = 5, color = "darkslategray", alpha = 0.9) +
geom_abline(intercept = b0, slope = b1, color = "#ff0000", size = 2, alpha = 0.9) +
scale_color_viridis(option = "cividis", direction = -1) +
theme_empty +
  labs(y = "GPA", x = "Family Income (10,000s of dollars)")
```

---

# OLS error term

So OLS is just the best-fit line through your data

--

Remember: for any given $i$, we won't have that $GPA_i = \widehat{GPA}_i$, there's always some error

--

Why?

--

Our model isn't perfect, the people in our dataset (i.e. our sample) may not perfectly match up to the entire population of people

---

# OLS error term

There's .hi[a lot] of other stuff that determines GPAs!

--

We jam all that stuff into error term $\varepsilon_i$:
$$ \text{GPA}_i = \beta_0 + \beta_1 I_i + \beta_2 P_i + \beta_3 \text{SAT}_i + \beta_4 H_i + \varepsilon_i $$

--

So $\varepsilon_i$ contains all the determinants of GPA that we aren't explicitly addressing in our model

---

# OLS properties

OLS has one .hi[very] nice property relevant for this class:<sup>1</sup>.footnote[
<sup>1</sup> The other is that is has the minimum variance of all unbiased estimators but that's not super important for us.
]

--

.hi-blue[Unbiasedness:] $E[\hat{\beta}] = \beta$

---

# OLS properties

.hi-blue[Unbiasedness:] $E[\hat{\beta}] = \beta$

On average, our estimate $\hat{\beta}$ exactly equals the .hi[true] $\beta$

--

The key is .hi-red[on average:] we are estimating our model using only some sample of the data

--

The estimated $\beta$ won't exactly be right for the entire population, but on average, we expect it to match

--

Let's see in an example where we only have a subsample of the full population of data

---

# OLS properties


.pull-left[

```{R, pop1, echo = F, fig.width = 4, fig.height = 4, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col)) +
geom_point(color = "darkslategray", size = 10) +
theme_blank
```

.center[**Population**]

]

--

.pull-right[

```{R, scatter1, echo = F, fig.width = 4, fig.height = 3, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#3C93DC", size = 3
) +
geom_point(color = "darkslategray", size = 6) +
theme_blank
```

.center[**Population relationship**]

$$ y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i $$

$$ y_i = \beta_0 + \beta_1 x_i + u_i $$


]

---

.pull-left[


```{R, sample1, echo = F, fig.width = 4, fig.height = 4, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s1)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```

.center[**Sample 1:** 10 random individuals]


]

--

.pull-right[


```{R, sample1 scatter, echo = F, fig.width = 4, fig.height = 3, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s1), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```


.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm1$coefficients[1], 2)` + `r round(lm1$coefficients[2], 2)` x_i$

]

]

---
count: false

.pull-left[

```{R, sample2, echo = F, fig.width = 4, fig.height = 4, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s2)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```

.center[**Sample 2:** 10 random individuals]

]

.pull-right[

```{R, sample2 scatter, echo = F, fig.width = 4, fig.height = 3, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s2), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```

.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm2$coefficients[1], 2)` + `r round(lm2$coefficients[2], 2)` x_i$

]

]
---
count: false

.pull-left[

```{R, sample3, echo = F, fig.width = 4, fig.height = 4, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = row, y = col, shape = s3)) +
geom_point(color = "darkslategray", size = 10) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```

.center[**Sample 3:** 10 random individuals]

]

.pull-right[

```{R, sample3 scatter, echo = F, fig.width = 4, fig.height = 3, fig.align = 'center', dev = "svg"}
ggplot(data = pop_df, aes(x = x, y = y)) +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = red_pink, size = 3, alpha = 0.3
) +
geom_point(aes(shape = s3), color = "darkslategray", size = 6) +
geom_abline(
  intercept = lm1$coefficients[1], slope = lm1$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm2$coefficients[1], slope = lm2$coefficients[2],
  size = 2, linetype = 2, color = "black", alpha = 0.3
) +
geom_abline(
  intercept = lm3$coefficients[1], slope = lm3$coefficients[2],
  size = 2, linetype = 2, color = "black"
) +
scale_shape_manual(values = c(1, 19)) +
theme_blank
```

.center[

**Population relationship**
<br>
$y_i = `r round(lm0$coefficients[1], 2)` + `r round(lm0$coefficients[2], 2)` x_i + u_i$

**Sample relationship**
<br>
$\hat{y}_i = `r round(lm3$coefficients[1], 2)` + `r round(lm3$coefficients[2], 2)` x_i$

]

]

---
layout: false
class: clear, middle

Let's repeat this **1,000 times**.

(This exercise is called a (Monte Carlo) simulation.)

---

# Population *vs.* sample

```{R, simulation scatter, echo = F, dev = "png", dpi = 300, cache = T, fig.height = 3}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.02, color = "darkslategray") +
geom_point(data = pop_df, aes(x = x, y = y), size = 3, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#3C93DC", size = 1.5
) +
theme_blank
```

---

# Population *vs.* sample

**Question:** Why do we care about *population vs. sample*?

---

.pull-left[
```{R, simulation scatter2, echo = F, dev = "png", dpi = 300, cache = T, fig.height = 6, fig.fullwidth = T}
# Reshape sim_df
line_df <- tibble(
  intercept = sim_df %>% filter(term != "x") %>% select(estimate) %>% unlist(),
  slope = sim_df %>% filter(term == "x") %>% select(estimate) %>% unlist()
)
ggplot() +
geom_abline(data = line_df, aes(intercept = intercept, slope = slope), alpha = 0.01, size = 1) +
geom_point(data = pop_df, aes(x = x, y = y), size = 6, color = "darkslategray") +
geom_abline(
  intercept = lm0$coefficients[1], slope = lm0$coefficients[2],
  color = "#3C93DC", size = 3
) +
theme_blank
```
]

.pull-right[

On .hi-blue[average], our regression lines match the population line very nicely

However, .hi[individual lines] (samples) can really miss the mark

Differences between individual samples and the population lead to **uncertainty** for us in the true effect

]

---

# Population *vs.* sample

**Answer:** Uncertainty matters!

--

$\hat{\beta}$ itself is random, it will depend on the sample of data we have

--

When we take a sample and run a regression, we don't know if it's a 'good' sample ( $\hat{\beta}$ is close to $\beta$) or a 'bad sample' (our sample differs greatly from the population)

---

# Unbiasedness

For OLS to be unbiased and give us, on average, the causal effect of some X on some Y we need a few assumptions to hold

--

Whether or not these assumptions are true is why you often hear *correlation is not causation*

--

If we want some $\hat{\beta}_1$ on a variable $x$ to be unbiased we need the following to be true:
$$E[x \varepsilon] = 0 \quad \leftrightarrow \quad \text{correlation}(x,\varepsilon) = 0$$

--

The variable you are interested in .hi[cannot] be correlated with the error term

---

# Unbiasedness

The variable you are interested in .hi[cannot] be correlated with the error term

--

What does this mean in words?

--

The error term contains all variables that determine $y$, but we *omitted* from our model

--

We are assuming that our variable of interest, x, is not correlated with any of these omitted variable

--

If x is correlated with any of them, then we will have something called .hi[omitted variable bias]

---

# Omitted variable bias

Here's an intuitive example

--

Suppose we wanted to understand the effect of lead exposure $P$ on GPAs

--

lead harm's children's brain development, especially before age 6

--

We should expect early-life lead exposure to reduce future GPAs

---

# Omitted variable bias

Our model might look like:
$$\text{GPA}_i = \beta_0 + \beta_1 \text{P}_i + \varepsilon_i$$

--

We want to know $\beta_1$

--

What would happen if we took a sample of *real world data* and used OLS to estimate $\hat{\beta}_1$?

---

# Omitted variable bias

We would have omitted variable bias

--

Why? What are some examples?

--

.hi[Who] is more likely to be exposed to lead?

--

Poorer families likely have more lead exposure, why?

--

Richer families can move away, pay to replace lead paint, lead pipes, etc

--

This means lead exposure is correlated with lower income

---

# Omitted variable bias

Why does this correlation cause us problems?

--

Family income *also* matters for GPA, it is in $\varepsilon_i$, so our assumption that $\text{correlation}(x,\varepsilon) = 0$ is violated

--

Children from richer families tend to have higher GPAs

---

# Omitted variable bias

If we just look at the effect of lead exposure on GPAs without addressing its correlation with income, lead exposure will look worse than it actually is

--

This is because our data on lead exposure is also proxying for income (since $\text{correlation}(x,\varepsilon) = 0$ )

--

So $\hat{beta}_1$ will pick up the effect of both!

--

Our estimate $\hat{\beta}_1$ is .hi[biased] and overstates the negative effects of lead

---

# Omitted variable bias

How do we fix this bias?

--

Make income not omitted: control for it in our model

--

If we have data on family income $I$ we can instead write our model as:
$$\text{GPA}_i = \beta_0 + \beta_1 \text{P}_i + \beta_2 \text{I}_i + \varepsilon_i$$

$I$ is no longer omitted

--

Independent variables in our model that we include to address bias are called .hi[controls]

---

class: inverse, center, middle
name: reg_in_r

# Regression in R

<html><div style='float:left'></div><hr color='#EB811B' size=1px width=796px></html>

---

# How do we actually run regressions?

Now we are going to learn how to run regressions in R

--

There's a lot of regression packages, built-in is `lm`, or you can also use a new one called `fixest`

--

We will also be using the `broom` package to make our output look nice

--

They work almost identically


---

# Using regression packages


To run a regression we need 3 things:

--

1. The package that will run the regression
2. Our regression formula
3. The dataframe that contains our data

--

In general, we will always run a regression like this: `package_name(formula_here, data = dataframe_here)`

--

You can then store the output by assigning it to a variable: `results = package_name(formula_here, data = dataframe_here)`

---

# Using regression packages

Let's start by using the built-in `lm` package along with the `starwars` dataset

```{r}
starwars
```

---

# Using regression packages

Suppose we wanted to see what was the effect of height on mass:
$$ mass_i = \beta_0 + \beta_1 height_i + \varepsilon_i $$

--

We can run the following code:

```{r}
# package_name(formula_here, data = dataframe_here)
lm(mass ~ height, data = starwars)
```

---

# Using regression packages

We can clean up the output a bit by piping it to `broom::tidy`:

```{r}
# package_name(formula_here, data = dataframe_here)
lm(mass ~ height, data = starwars) %>%
  broom::tidy()
```

---

# Using regression packages

```{r, echo = F}
# package_name(formula_here, data = dataframe_here)
starwars1 <- lm(mass ~ height, data = starwars) %>%
  broom::tidy()
```

The first column gives us our estimates: $\hat{\beta_0}, \hat{\beta}_1$

--

The last column is the .hi[p-value]: the probability that we would have gotten an estimate at least that large, if the *true* value was actually 0

Smaller values generally mean it is more likely that height has an effect on mass: the probability that we could have gotten our estimated value if height didn't matter is very low


---

# Interpreting coefficient estimates

```{r, echo = F}
starwars1
```

What do these coefficient estimates mean?

--

The estimate for the $\beta$ on height means: if we increase height by 1cm, mass increases by 0.639kg

--

This just comes from our previous example where $\beta_1 = \frac{\partial mass_i}{\partial height_i}$

---

# Interpreting coefficient estimates

```{r, echo = F}
starwars1
```

How do we interpret $\beta_0$, the estimate of the *intercept*?

--

Well, it is just the estimated mass, given someone had zero height: 
$$\hat{\beta}_0 = \hat{\beta}_0 + \hat{\beta}_1 \times 0$$

--

It's kind of a nonsense interpretation here since no one has zero height

--

Generally we don't read too much into the intercept terms

---

# Interpreting coefficient estimates

Now, what if we changed our model a bit so it was instead:
$$\log(\text{mass}_i) = \beta_0 + \beta_1 \log(\text{height}_i) + \varepsilon_i$$

What does $\beta_1$ mean now?

--

$$\beta_1 = \frac{\partial \log(\text{mass}_i)}{\partial \log(\text{height}_i)}$$

--

But we can rewrite this as:
$$\beta_1 = \frac{\partial \log(\text{mass}_i)}{\partial \log(\text{height}_i)} = \frac{\partial \log(\text{mass}_i)}{\partial \text{mass}_i}\frac{\partial \text{mass}_i}{\partial \text{height}_i}\frac{\partial \text{height}_i}{\partial \log(\text{height}_i)}$$

---

# Interpreting coefficient estimates

$$\beta_1 = \frac{\partial \log(\text{mass}_i)}{\partial \log(\text{height}_i)} = \frac{\partial \log(\text{mass}_i)}{\partial \text{mass}_i}\frac{\partial \text{mass}_i}{\partial \text{height}_i}\frac{\partial \text{height}_i}{\partial \log(\text{height}_i)}$$

And this is equal to:
$$\beta_1 = \frac{\partial \log(\text{mass}_i)}{\partial \log(\text{height}_i)} = \frac{1}{\text{mass}_i}\frac{\partial \text{mass}_i}{\partial \text{height}_i}\frac{\text{height}_i}{1}$$

--

And finally:
$$\beta_1 = \frac{\partial \log(\text{mass}_i)}{\partial \log(\text{height}_i)} = \frac{\text{height}_i}{\text{mass}_i}\frac{\partial \text{mass}_i}{\partial \text{height}_i}$$

which is the definition of the elasticity of mass with respect to height

---

# Interpreting coefficient estimates

$$\log(\text{mass}_i) = \beta_0 + \beta_1 \log(\text{height}_i) + \varepsilon_i$$

In a *log-log* model, $\beta$ 1 tells us the percent change in mass, given a percent change in height

--

Let's run the regression:

```{r}
# package_name(formula_here, data = dataframe_here)
lm(log(mass) ~ log(height), data = starwars) %>%
  broom::tidy()
```

---

# Interpreting coefficient estimates

```{r}
# package_name(formula_here, data = dataframe_here)
lm(log(mass) ~ log(height), data = starwars) %>%
  broom::tidy()
```

A 1% increase in height is associated with a 1.58% increase in mass!